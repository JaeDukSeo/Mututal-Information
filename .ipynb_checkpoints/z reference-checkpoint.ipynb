{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     75,
     110,
     139,
     194,
     281,
     390,
     464,
     519,
     546,
     558,
     587,
     608
    ]
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Copyright (C) 2016 Paul Brodersen <paulbrodersen+entropy_estimators@gmail.com>\n",
    "\n",
    "# Author: Paul Brodersen <paulbrodersen+entropy_estimators@gmail.com>\n",
    "\n",
    "# This program is free software; you can redistribute it and/or\n",
    "# modify it under the terms of the GNU General Public License\n",
    "# as published by the Free Software Foundation; either version 3\n",
    "# of the License, or (at your option) any later version.\n",
    "\n",
    "# This program is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU General Public License for more details.\n",
    "\n",
    "# You should have received a copy of the GNU General Public License\n",
    "# along with this program. If not, see <http://www.gnu.org/licenses/>.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "TODO:\n",
    "- make python3 compatible\n",
    "- fix code for p-norm 1 and 2 (norm argument currently ignored)\n",
    "- write test for get_pid()\n",
    "- get_pmi() with normalisation fails test\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.special import gamma, digamma\n",
    "from scipy.stats   import multivariate_normal, rankdata\n",
    "\n",
    "\n",
    "log = np.log   # i.e. information measures are in nats\n",
    "# log = np.log2  # i.e. information measures are in bits\n",
    "\n",
    "\n",
    "def unit_interval(arr):\n",
    "    return (arr - np.nanmin(arr, axis=0)[None,:]) / (np.nanmax(arr, axis=0) - np.nanmin(arr, axis=0))\n",
    "\n",
    "\n",
    "def rank(arr):\n",
    "    return np.apply_along_axis(rankdata, 0, arr)\n",
    "\n",
    "\n",
    "def det(array_or_scalar):\n",
    "    if array_or_scalar.size > 1:\n",
    "        return np.linalg.det(array_or_scalar)\n",
    "    else:\n",
    "        return array_or_scalar\n",
    "\n",
    "\n",
    "def get_h_mvn(x):\n",
    "\n",
    "    \"\"\"\n",
    "    Computes the entropy of a multivariate Gaussian distribution:\n",
    "    H(X) = (1/2) * log((2 * pi * e)^d * det(cov(X)))\n",
    "    Arguments:\n",
    "    ----------\n",
    "    x: (n, d) ndarray\n",
    "        n samples from a d-dimensional multivariate normal distribution\n",
    "    Returns:\n",
    "    --------\n",
    "    h: float\n",
    "        entropy H(X)\n",
    "    \"\"\"\n",
    "\n",
    "    d = x.shape[1]\n",
    "    h  = 0.5 * log((2 * np.pi * np.e)**d * det(np.cov(x.T)))\n",
    "    return h\n",
    "\n",
    "\n",
    "def get_mi_mvn(x, y):\n",
    "    \"\"\"\n",
    "    Computes the mutual information I between two multivariate normal random\n",
    "    variables, X and Y:\n",
    "    I(X, Y) = H(X) + H(Y) - H(X, Y)\n",
    "    Arguments:\n",
    "    ----------\n",
    "    x, y: (n, d) ndarrays\n",
    "        n samples from d-dimensional multivariate normal distributions\n",
    "    Returns:\n",
    "    --------\n",
    "    mi: float\n",
    "        mutual information I(X, Y)\n",
    "    \"\"\"\n",
    "\n",
    "    d = x.shape[1]\n",
    "\n",
    "    # hx  = 0.5 * log((2 * np.pi * np.e)**d     * det(np.cov(x.T)))\n",
    "    # hy  = 0.5 * log((2 * np.pi * np.e)**d     * det(np.cov(y.T)))\n",
    "    # hxy = 0.5 * log((2 * np.pi * np.e)**(2*d) * det(np.cov(x.T, y=y.T)))\n",
    "    # mi = hx + hy - hxy\n",
    "\n",
    "    # hx  = 0.5 * log(det(2*np.pi*np.e*np.cov(x.T)))\n",
    "    # hy  = 0.5 * log(det(2*np.pi*np.e*np.cov(y.T)))\n",
    "    # hxy = 0.5 * log(det(2*np.pi*np.e*np.cov(np.c_[x,y].T)))\n",
    "    hx  = get_h_mvn(x)\n",
    "    hy  = get_h_mvn(y)\n",
    "    hxy = get_h_mvn(np.c_[x,y])\n",
    "    mi = hx + hy - hxy\n",
    "\n",
    "    # mi = 0.5 * (log(det(np.cov(x.T))) + log(det(np.cov(y.T))) - log(det(np.cov(np.c_[x,y].T))))\n",
    "\n",
    "    return mi\n",
    "\n",
    "def get_pmi_mvn(x, y, z):\n",
    "    \"\"\"\n",
    "    Computes the partial mutual information PMI between two multivariate normal random\n",
    "    variables, X and Y, while conditioning on a third MVN RV, Z:\n",
    "    I(X;Y|Z) = H(X,Z) + H(Y,Z) - H(X, Y, Z) - H(Z)\n",
    "    where:\n",
    "    H(Z)     = (1/2) * log(det(2 * pi * e * cov(Z)))\n",
    "    H(X,Z)   = (1/2) * log(det(2 * pi * e * cov(XZ)))\n",
    "    H(Y,Z)   = (1/2) * log(det(2 * pi * e * cov(YZ)))\n",
    "    H(X,Y,Z) = (1/2) * log(det(2 * pi * e * cov(XYZ)))\n",
    "    Arguments:\n",
    "    ----------\n",
    "    x, y, z: (n, d) ndarrays\n",
    "        n samples from d-dimensional multivariate normal distributions\n",
    "    Returns:\n",
    "    --------\n",
    "    pmi: float\n",
    "        partial mutual information I(X;Y|Z)\n",
    "    \"\"\"\n",
    "\n",
    "    d = x.shape[1]\n",
    "    hz   = 0.5 * log((2 * np.pi * np.e)**d     * det(np.cov(z.T)))\n",
    "    hxz  = 0.5 * log((2 * np.pi * np.e)**(2*d) * det(np.cov(x.T, y=z.T)))\n",
    "    hyz  = 0.5 * log((2 * np.pi * np.e)**(2*d) * det(np.cov(y.T, y=z.T)))\n",
    "    hxyz = 0.5 * log((2 * np.pi * np.e)**(3*d) * det(np.cov(np.c_[x,y,z].T)))\n",
    "\n",
    "    pmi = hxz + hyz - hxyz - hz\n",
    "    return pmi\n",
    "\n",
    "def get_h(x, k=1, norm=np.inf, min_dist=0.):\n",
    "    \"\"\"\n",
    "    Estimates the entropy H of a random variable x (in nats) based on\n",
    "    the kth-nearest neighbour distances between point samples.\n",
    "    @reference:\n",
    "    Kozachenko, L., & Leonenko, N. (1987). Sample estimate of the entropy of a random vector. Problemy Peredachi Informatsii, 23(2), 9â€“16.\n",
    "    Arguments:\n",
    "    ----------\n",
    "    x: (n, d) ndarray\n",
    "        n samples from a d-dimensional multivariate distribution\n",
    "    k: int (default 1)\n",
    "        kth nearest neighbour to use in density estimate;\n",
    "        imposes smoothness on the underlying probability distribution\n",
    "    norm: 1, 2, or np.inf (default np.inf)\n",
    "        p-norm used when computing k-nearest neighbour distances\n",
    "            1: absolute-value norm\n",
    "            2: euclidean norm\n",
    "            3: max norm\n",
    "    min_dist: float (default 0.)\n",
    "        minimum distance between data points;\n",
    "        smaller distances will be capped using this value\n",
    "    Returns:\n",
    "    --------\n",
    "    h: float\n",
    "        entropy H(X)\n",
    "    \"\"\"\n",
    "\n",
    "    n, d = x.shape\n",
    "\n",
    "    # volume of the d-dimensional unit ball...\n",
    "    # if norm == np.inf: # max norm:\n",
    "    #     log_c_d = 0\n",
    "    # elif norm == 2: # euclidean norm\n",
    "    #     log_c_d = (d/2.) * log(np.pi) -log(gamma(d/2. +1))\n",
    "    # elif norm == 1:\n",
    "    #     raise NotImplementedError\n",
    "    # else:\n",
    "    #     raise NotImplementedError(\"Variable 'norm' either 1, 2 or np.inf\")\n",
    "    log_c_d = 0.\n",
    "\n",
    "    kdtree = cKDTree(x)\n",
    "\n",
    "    # query all points -- k+1 as query point also in initial set\n",
    "    # distances, idx = kdtree.query(x, k + 1, eps=0, p=norm)\n",
    "    distances, idx = kdtree.query(x, k + 1, eps=0, p=np.inf)\n",
    "    distances = distances[:, -1]\n",
    "\n",
    "    # enforce non-zero distances\n",
    "    distances[distances < min_dist] = min_dist\n",
    "\n",
    "    sum_log_dist = np.sum(log(2*distances)) # where did the 2 come from? radius -> diameter\n",
    "    h = -digamma(k) + digamma(n) + log_c_d + (d / float(n)) * sum_log_dist\n",
    "\n",
    "    return h\n",
    "\n",
    "def get_mi(x, y, k=1, normalize=None, norm=np.inf, estimator='ksg'):\n",
    "    \"\"\"\n",
    "    Estimates the mutual information (in nats) between two point clouds, x and y,\n",
    "    in a D-dimensional space.\n",
    "    I(X,Y) = H(X) + H(Y) - H(X,Y)\n",
    "    @reference:\n",
    "    Kraskov, Stoegbauer & Grassberger (2004). Estimating mutual information. PHYSICAL REVIEW E 69, 066138\n",
    "    Arguments:\n",
    "    ----------\n",
    "    x, y: (n, d) ndarray\n",
    "        n samples from d-dimensional multivariate distributions\n",
    "    k: int (default 1)\n",
    "        kth nearest neighbour to use in density estimate;\n",
    "        imposes smoothness on the underlying probability distribution\n",
    "    normalize: function or None (default None)\n",
    "        if a function, the data pre-processed with the function before the computation\n",
    "    norm: 1, 2, or np.inf (default np.inf)\n",
    "        p-norm used when computing k-nearest neighbour distances\n",
    "            1: absolute-value norm\n",
    "            2: euclidean norm\n",
    "            3: max norm\n",
    "    min_dist: float (default 0.)\n",
    "        minimum distance between data points;\n",
    "        smaller distances will be capped using this value\n",
    "    estimator: 'ksg' or 'naive' (default 'ksg')\n",
    "        'ksg'  : see Kraskov, Stoegbauer & Grassberger (2004) Estimating mutual information, eq(8).\n",
    "        'naive': entropies are calculated individually using the Kozachenko-Leonenko estimator implemented in get_h()\n",
    "    Returns:\n",
    "    --------\n",
    "    mi: float\n",
    "        mutual information I(X,Y)\n",
    "    \"\"\"\n",
    "\n",
    "    if normalize:\n",
    "        x = normalize(x)\n",
    "        y = normalize(y)\n",
    "\n",
    "    # construct state array for the joint process:\n",
    "    xy = np.c_[x,y]\n",
    "\n",
    "    if estimator == 'naive':\n",
    "        # compute individual entropies\n",
    "        hx  = get_h(x,  k=k, norm=norm)\n",
    "        hy  = get_h(y,  k=k, norm=norm)\n",
    "        hxy = get_h(xy, k=k, norm=norm)\n",
    "\n",
    "        # compute mi\n",
    "        mi = hx + hy - hxy\n",
    "\n",
    "    elif estimator == 'ksg':\n",
    "\n",
    "        # store data pts in kd-trees for efficient nearest neighbour computations\n",
    "        # TODO: choose a better leaf size\n",
    "        x_tree  = cKDTree(x)\n",
    "        y_tree  = cKDTree(y)\n",
    "        xy_tree = cKDTree(xy)\n",
    "\n",
    "        # kth nearest neighbour distances for every state\n",
    "        # query with k=k+1 to return the nearest neighbour, not counting the data point itself\n",
    "        # dist, idx = xy_tree.query(xy, k=k+1, p=norm)\n",
    "        dist, idx = xy_tree.query(xy, k=k+1, p=np.inf)\n",
    "        epsilon = dist[:, -1]\n",
    "\n",
    "        # for each point, count the number of neighbours\n",
    "        # whose distance in the x-subspace is strictly < epsilon\n",
    "        # repeat for the y subspace\n",
    "        n = len(x)\n",
    "        nx = np.empty(n, dtype=np.int)\n",
    "        ny = np.empty(n, dtype=np.int)\n",
    "        for ii in range(n):\n",
    "            # nx[ii] = len(x_tree.query_ball_point(x_tree.data[ii], r=epsilon[ii], p=norm)) - 1\n",
    "            # ny[ii] = len(y_tree.query_ball_point(y_tree.data[ii], r=epsilon[ii], p=norm)) - 1\n",
    "            nx[ii] = len(x_tree.query_ball_point(x_tree.data[ii], r=epsilon[ii], p=np.inf)) - 1\n",
    "            ny[ii] = len(y_tree.query_ball_point(y_tree.data[ii], r=epsilon[ii], p=np.inf)) - 1\n",
    "\n",
    "        mi = digamma(k) - np.mean(digamma(nx+1) + digamma(ny+1)) + digamma(n) # version (1)\n",
    "        # mi = digamma(k) -1./k -np.mean(digamma(nx) + digamma(ny)) + digamma(n) # version (2)\n",
    "\n",
    "    elif estimator == 'lnc':\n",
    "        # TODO: (only if you can find some decent explanation on how to set alpha!)\n",
    "        raise NotImplementedError(\"Estimator is one of 'naive', 'ksg'; currently: {}\".format(estimator))\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\"Estimator is one of 'naive', 'ksg'; currently: {}\".format(estimator))\n",
    "\n",
    "    return mi\n",
    "\n",
    "def get_pmi(x, y, z, k=1, normalize=None, norm=np.inf, estimator='fp'):\n",
    "    \"\"\"\n",
    "    Estimates the partial mutual information (in nats), i.e. the\n",
    "    information between two point clouds, x and y, in a D-dimensional\n",
    "    space while conditioning on a third variable z.\n",
    "    I(X,Y|Z) = H(X,Z) + H(Y,Z) - H(X,Y,Z) - H(Z)\n",
    "    The estimators are based on:\n",
    "    @reference:\n",
    "    Frenzel & Pombe (2007) Partial mutual information for coupling analysis of multivariate time series\n",
    "    Poczos & Schneider (2012) Nonparametric Estimation of Conditional Information and Divergences\n",
    "    Arguments:\n",
    "    ----------\n",
    "    x, y, z: (n, d) ndarray\n",
    "        n samples from d-dimensional multivariate distributions\n",
    "    k: int (default 1)\n",
    "        kth nearest neighbour to use in density estimate;\n",
    "        imposes smoothness on the underlying probability distribution\n",
    "    normalize: function or None (default None)\n",
    "        if a function, the data pre-processed with the function before the computation\n",
    "    norm: 1, 2, or np.inf (default np.inf)\n",
    "        p-norm used when computing k-nearest neighbour distances\n",
    "            1: absolute-value norm\n",
    "            2: euclidean norm\n",
    "            3: max norm\n",
    "    estimator: 'fp', 'ps' or 'naive' (default 'fp')\n",
    "        'naive': entropies are calculated individually using the Kozachenko-Leonenko estimator implemented in get_h()\n",
    "        'fp'   : Frenzel & Pombe estimator (effectively the KSG-estimator for mutual information)\n",
    "    Returns:\n",
    "    --------\n",
    "    pmi: float\n",
    "        partial mutual information I(X,Y;Z)\n",
    "    \"\"\"\n",
    "\n",
    "    if normalize:\n",
    "        x = normalize(x)\n",
    "        y = normalize(y)\n",
    "        z = normalize(z)\n",
    "\n",
    "    # construct state array for the joint processes:\n",
    "    xz  = np.c_[x,z]\n",
    "    yz  = np.c_[y,z]\n",
    "    xyz = np.c_[x,y,z]\n",
    "\n",
    "    if estimator == 'naive':\n",
    "        # compute individual entropies\n",
    "        # TODO: pass in min_dist\n",
    "        hz   = get_h(z,   k=k, norm=norm)\n",
    "        hxz  = get_h(xz,  k=k, norm=norm)\n",
    "        hyz  = get_h(yz,  k=k, norm=norm)\n",
    "        hxyz = get_h(xyz, k=k, norm=norm)\n",
    "\n",
    "        pmi =  hxz + hyz - hxyz - hz\n",
    "\n",
    "    elif estimator == 'fp':\n",
    "\n",
    "        # construct k-d trees\n",
    "        z_tree   = cKDTree(z)\n",
    "        xz_tree  = cKDTree(xz)\n",
    "        yz_tree  = cKDTree(yz)\n",
    "        xyz_tree = cKDTree(xyz)\n",
    "\n",
    "        # kth nearest neighbour distances for every state\n",
    "        # query with k=k+1 to return the nearest neighbour, not the data point itself\n",
    "        # dist, idx = xyz_tree.query(xyz, k=k+1, p=norm)\n",
    "        dist, idx = xyz_tree.query(xyz, k=k+1, p=np.inf)\n",
    "        epsilon = dist[:, -1]\n",
    "\n",
    "        # for each point, count the number of neighbours\n",
    "        # whose distance in the relevant subspace is strictly < epsilon\n",
    "        n = len(x)\n",
    "        nxz = np.empty(n, dtype=np.int)\n",
    "        nyz = np.empty(n, dtype=np.int)\n",
    "        nz  = np.empty(n, dtype=np.int)\n",
    "\n",
    "        for ii in range(n):\n",
    "            # nz[ii]  = len( z_tree.query_ball_point( z_tree.data[ii], r=epsilon[ii], p=norm)) - 1\n",
    "            # nxz[ii] = len(xz_tree.query_ball_point(xz_tree.data[ii], r=epsilon[ii], p=norm)) - 1\n",
    "            # nyz[ii] = len(yz_tree.query_ball_point(yz_tree.data[ii], r=epsilon[ii], p=norm)) - 1\n",
    "            nz[ii]  = len( z_tree.query_ball_point( z_tree.data[ii], r=epsilon[ii], p=np.inf)) - 1\n",
    "            nxz[ii] = len(xz_tree.query_ball_point(xz_tree.data[ii], r=epsilon[ii], p=np.inf)) - 1\n",
    "            nyz[ii] = len(yz_tree.query_ball_point(yz_tree.data[ii], r=epsilon[ii], p=np.inf)) - 1\n",
    "\n",
    "        pmi = digamma(k) + np.mean(digamma(nz +1) -digamma(nxz +1) -digamma(nyz +1))\n",
    "\n",
    "    elif estimator == 'ps':\n",
    "        # (I am fairly sure that) this is the correct implementation of the estimator,\n",
    "        # but the estimators is just crap.\n",
    "\n",
    "        # construct k-d trees\n",
    "        xz_tree  = cKDTree(xz,  leafsize=2*k)\n",
    "        yz_tree  = cKDTree(yz,  leafsize=2*k)\n",
    "\n",
    "        # determine k-nn distances\n",
    "        n = len(x)\n",
    "        rxz = np.empty(n, dtype=np.int)\n",
    "        ryz = np.empty(n, dtype=np.int)\n",
    "\n",
    "        # rxz, dummy = xz_tree.query(xz, k=k+1, p=norm) # +1 to account for distance to itself\n",
    "        # ryz, dummy = yz_tree.query(xz, k=k+1, p=norm) # +1 to account for distance to itself; xz NOT a typo\n",
    "        rxz, dummy = xz_tree.query(xz, k=k+1, p=np.inf) # +1 to account for distance to itself\n",
    "        ryz, dummy = yz_tree.query(xz, k=k+1, p=np.inf) # +1 to account for distance to itself; xz NOT a typo\n",
    "\n",
    "        pmi = yz.shape[1] * np.mean(log(ryz[:,-1]) - log(rxz[:,-1])) # + log(n) -log(n-1) -1.\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\"Estimator one of 'naive', 'fp', 'ps'; currently: {}\".format(estimator))\n",
    "\n",
    "    return pmi\n",
    "\n",
    "def get_imin(x1, x2, y, k=1, normalize=None, norm=np.inf):\n",
    "    \"\"\"\n",
    "    Estimates the average specific information (in nats) between a random variable Y\n",
    "    and two explanatory variables, X1 and X2.\n",
    "    I_min(Y; X1, X2) = \\sum_{y \\in Y} p(y) min_{X \\in {X1, X2}} I_spec(y; X)\n",
    "    where\n",
    "    I_spec(y; X) = \\sum_{x \\in X} p(x|y) \\log(p(y|x) / p(x))\n",
    "    @reference:\n",
    "    Williams & Beer (2010). Nonnegative Decomposition of Multivariate Information. arXiv:1004.2515v1\n",
    "    Kraskov, Stoegbauer & Grassberger (2004). Estimating mutual information. PHYSICAL REVIEW E 69, 066138\n",
    "    Arguments:\n",
    "    ----------\n",
    "    x1, x2, y: (n, d) ndarray\n",
    "        n samples from d-dimensional multivariate distributions\n",
    "    k: int (default 1)\n",
    "        kth nearest neighbour to use in density estimate;\n",
    "        imposes smoothness on the underlying probability distribution\n",
    "    normalize: function or None (default None)\n",
    "        if a function, the data pre-processed with the function before the computation\n",
    "    norm: 1, 2, or np.inf (default np.inf)\n",
    "        p-norm used when computing k-nearest neighbour distances\n",
    "            1: absolute-value norm\n",
    "            2: euclidean norm\n",
    "            3: max norm\n",
    "    Returns:\n",
    "    --------\n",
    "    i_min: float\n",
    "        average specific information I_min(Y; X1, X2)\n",
    "    \"\"\"\n",
    "\n",
    "    if normalize:\n",
    "        y = normalize(y)\n",
    "\n",
    "    y_tree  = cKDTree(y)\n",
    "\n",
    "    n = len(y)\n",
    "    i_spec = np.zeros((2, n))\n",
    "\n",
    "    for jj, x in enumerate([x1, x2]):\n",
    "\n",
    "        if normalize:\n",
    "            x = normalize(x)\n",
    "\n",
    "        # construct state array for the joint processes:\n",
    "        xy = np.c_[x,y]\n",
    "\n",
    "        # store data pts in kd-trees for efficient nearest neighbour computations\n",
    "        # TODO: choose a better leaf size\n",
    "        x_tree  = cKDTree(x)\n",
    "        xy_tree = cKDTree(xy)\n",
    "\n",
    "        # kth nearest neighbour distances for every state\n",
    "        # query with k=k+1 to return the nearest neighbour, not counting the data point itself\n",
    "        # dist, idx = xy_tree.query(xy, k=k+1, p=norm)\n",
    "        dist, idx = xy_tree.query(xy, k=k+1, p=np.inf)\n",
    "        epsilon = dist[:, -1]\n",
    "\n",
    "        # for each point, count the number of neighbours\n",
    "        # whose distance in the x-subspace is strictly < epsilon\n",
    "        # repeat for the y subspace\n",
    "        nx = np.empty(n, dtype=np.int)\n",
    "        ny = np.empty(n, dtype=np.int)\n",
    "        for ii in xrange(N):\n",
    "            # nx[ii] = len(x_tree.query_ball_point(x_tree.data[ii], r=epsilon[ii], p=norm)) - 1\n",
    "            # ny[ii] = len(y_tree.query_ball_point(y_tree.data[ii], r=epsilon[ii], p=norm)) - 1\n",
    "            nx[ii] = len(x_tree.query_ball_point(x_tree.data[ii], r=epsilon[ii], p=np.inf)) - 1\n",
    "            ny[ii] = len(y_tree.query_ball_point(y_tree.data[ii], r=epsilon[ii], p=np.inf)) - 1\n",
    "\n",
    "        i_spec[jj] = digamma(k) - digamma(nx+1) + digamma(ny+1) + digamma(n) # version (1)\n",
    "\n",
    "    i_min = np.mean(np.min(i_spec, 0))\n",
    "\n",
    "    return i_min\n",
    "\n",
    "def get_pid(x1, x2, y, k=1, normalize=None, norm=np.inf):\n",
    "\n",
    "    \"\"\"\n",
    "    Estimates the partial information decomposition (in nats) between a random variable Y\n",
    "    and two explanatory variables, X1 and X2.\n",
    "    I(X1, X2; Y) = synergy + unique_{X1} + unique_{X2} + redundancy\n",
    "    redundancy = I_{min}(X1, X2; Y)\n",
    "    unique_{X1} = I(X1; Y) - redundancy\n",
    "    unique_{X2} = I(X2; Y) - redundancy\n",
    "    synergy = I(X1, X2; Y) - I(X1; Y) - I(X2; Y) + redundancy\n",
    "    The estimator is based on:\n",
    "    @reference:\n",
    "    Williams & Beer (2010). Nonnegative Decomposition of Multivariate Information. arXiv:1004.2515v1\n",
    "    Kraskov, Stoegbauer & Grassberger (2004). Estimating mutual information. PHYSICAL REVIEW E 69, 066138\n",
    "    For a critique of I_min as a redundancy measure, see\n",
    "    Bertschinger et al. (2012). Shared Information â€“ New Insights and Problems in Decomposing Information in Complex Systems. arXiv:1210.5902v1\n",
    "    Griffith & Koch (2014). Quantifying synergistic mutual information. arXiv:1205.4265v6\n",
    "    Arguments:\n",
    "    ----------\n",
    "    x1, x2, y: (n, d) ndarray\n",
    "        n samples from d-dimensional multivariate distributions\n",
    "    k: int (default 1)\n",
    "        kth nearest neighbour to use in density estimate;\n",
    "        imposes smoothness on the underlying probability distribution\n",
    "    normalize: function or None (default None)\n",
    "        if a function, the data pre-processed with the function before the computation\n",
    "    norm: 1, 2, or np.inf (default np.inf)\n",
    "        p-norm used when computing k-nearest neighbour distances\n",
    "            1: absolute-value norm\n",
    "            2: euclidean norm\n",
    "            3: max norm\n",
    "    Returns:\n",
    "    --------\n",
    "    synergy: float\n",
    "        information about Y encoded by the joint state of x1 and x2\n",
    "    unique_x1: float\n",
    "        information about Y encoded uniquely by x1\n",
    "    unique_x2: float\n",
    "        information about Y encoded uniquely by x2\n",
    "    redundancy: float\n",
    "        information about Y encoded by either x1 or x2\n",
    "    \"\"\"\n",
    "\n",
    "    mi_x1y     = get_mi(x1,            y, k=k, normalize=normalize, norm=norm)\n",
    "    mi_x2y     = get_mi(x2,            y, k=k, normalize=normalize, norm=norm)\n",
    "    mi_x1x2y   = get_mi(np.c_[x1, x2], y, k=k, normalize=normalize, norm=norm)\n",
    "    redundancy = get_imin(x1, x2,      y, k=k, normalize=normalize, norm=norm)\n",
    "\n",
    "    unique_x1 = mi_x1y - redundancy\n",
    "    unique_x2 = mi_x2y - redundancy\n",
    "    synergy = mi_x1x2y - mi_x1y - mi_x2y + redundancy\n",
    "\n",
    "    return synergy, unique_x1, unique_x2, redundancy\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "def get_mvn_data(total_rvs, dimensionality=2, scale_sigma_offdiagonal_by=1., total_samples=1000):\n",
    "    data_space_size = total_rvs * dimensionality\n",
    "\n",
    "    # initialise distribution\n",
    "    mu = np.random.randn(data_space_size)\n",
    "    sigma = np.random.rand(data_space_size, data_space_size)\n",
    "    # sigma = 1. + 0.5*np.random.randn(data_space_size, data_space_size)\n",
    "\n",
    "    # ensures that sigma is positive semi-definite\n",
    "    sigma = np.dot(sigma.transpose(), sigma)\n",
    "\n",
    "    # scale off-diagonal entries -- might want to change that to block diagonal entries\n",
    "    # diag = np.diag(sigma).copy()\n",
    "    # sigma *= scale_sigma_offdiagonal_by\n",
    "    # sigma[np.diag_indices(len(diag))] = diag\n",
    "\n",
    "    # scale off-block diagonal entries\n",
    "    d = dimensionality\n",
    "    for ii, jj in itertools.product(range(total_rvs), repeat=2):\n",
    "        if ii != jj:\n",
    "            sigma[d*ii:d*(ii+1), d*jj:d*(jj+1)] *= scale_sigma_offdiagonal_by\n",
    "\n",
    "    # get samples\n",
    "    samples = multivariate_normal(mu, sigma).rvs(total_samples)\n",
    "\n",
    "    return [samples[:,ii*d:(ii+1)*d] for ii in range(total_rvs)]\n",
    "\n",
    "def test_get_h(k=5, norm=np.inf):\n",
    "    X, = get_mvn_data(total_rvs=1,\n",
    "                      dimensionality=2,\n",
    "                      scale_sigma_offdiagonal_by=1.,\n",
    "                      total_samples=1000)\n",
    "\n",
    "    analytic = get_h_mvn(X)\n",
    "    kozachenko = get_h(X, k=k, norm=norm)\n",
    "\n",
    "    print(\"analytic result: {:.5f}\".format(analytic))\n",
    "    print(\"K-L estimator:   {:.5f}\".format(kozachenko))\n",
    "\n",
    "def test_get_mi(k=5, normalize=None, norm=np.inf):\n",
    "\n",
    "    X, Y = get_mvn_data(total_rvs=2,\n",
    "                        dimensionality=2,\n",
    "                        scale_sigma_offdiagonal_by=1., # 0.1, 0.\n",
    "                        total_samples=10000)\n",
    "\n",
    "    # solutions\n",
    "    analytic = get_mi_mvn(X, Y)\n",
    "    naive = get_mi(X, Y, k=k, normalize=normalize, norm=norm, estimator='naive')\n",
    "    ksg   = get_mi(X, Y, k=k, normalize=normalize, norm=norm, estimator='ksg')\n",
    "\n",
    "    print(\"analytic result:  {:.5f}\".format(analytic))\n",
    "    print(\"naive estimator:  {:.5f}\".format(naive))\n",
    "    print(\"KSG estimator:    {:.5f}\".format(ksg))\n",
    "    print\n",
    "\n",
    "    print(\"naive - analytic: {:.5f}\".format(naive - analytic))\n",
    "    print(\"ksg   - analytic: {:.5f}\".format(ksg   - analytic))\n",
    "    print\n",
    "\n",
    "    print(\"naive / analytic: {:.5f}\".format(naive / analytic))\n",
    "    print(\"ksg   / analytic: {:.5f}\".format(ksg   / analytic))\n",
    "    print\n",
    "\n",
    "    # for automated testing:\n",
    "    assert np.isclose(analytic, naive, rtol=0.1, atol=0.1), \"Naive MI estimate strongly differs from expectation!\"\n",
    "    assert np.isclose(analytic, ksg,   rtol=0.1, atol=0.1), \"KSG MI estimate strongly differs from expectation!\"\n",
    "\n",
    "def test_get_pmi(k=5, normalize=None, norm=np.inf):\n",
    "\n",
    "    X, Y, Z = get_mvn_data(total_rvs=3,\n",
    "                           dimensionality=2,\n",
    "                           scale_sigma_offdiagonal_by=1.,\n",
    "                           total_samples=10000)\n",
    "\n",
    "    # solutions\n",
    "    analytic = get_pmi_mvn(X, Y, Z)\n",
    "    naive    = get_pmi(X, Y, Z, k=k, normalize=normalize, norm=norm, estimator='naive')\n",
    "    fp       = get_pmi(X, Y, Z, k=k, normalize=normalize, norm=norm, estimator='fp')\n",
    "\n",
    "    print(\"analytic result : {:.5f}\".format(analytic))\n",
    "    print(\"naive estimator : {:.5f}\".format(naive))\n",
    "    print(\"FP estimator    : {:.5f}\".format(fp))\n",
    "    print\n",
    "\n",
    "    # for automated testing:\n",
    "    assert np.isclose(analytic, naive, rtol=0.5, atol=0.5), \"Naive MI estimate strongly differs from expectation!\"\n",
    "    assert np.isclose(analytic, fp,    rtol=0.5, atol=0.5), \"FP MI estimate strongly differs from expectation!\"\n",
    "\n",
    "def test_get_pid(k=5, normalize=None, norm=np.inf):\n",
    "    # rdn -> only redundant information\n",
    "\n",
    "    # unq -> only unique information\n",
    "\n",
    "    # xor -> only synergistic information\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'entropy_estimators'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f3c3abc4f19b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mentropy_estimators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontinuous\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# create some multivariate normal test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m X, = continous.get_mvn_data(total_rvs=1,        # number of random variables\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'entropy_estimators'"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from entropy_estimators import continuous\n",
    "\n",
    "# create some multivariate normal test data\n",
    "X, = continous.get_mvn_data(total_rvs=1,        # number of random variables\n",
    "                            dimensionality=2,   # dimensionality of each RV\n",
    "                            total_samples=1000) # samples\n",
    "\n",
    "# compute the entropy from the determinant of the multivariate normal distribution:\n",
    "analytic = continuous.get_h_mvn(X)\n",
    "\n",
    "# compute the entropy using the k-nearest neighbour approach\n",
    "# developed by Kozachenko and Leonenko (1987):\n",
    "kozachenko = continuous.get_h(X, k=5)\n",
    "\n",
    "print(\"analytic result: {: .5f}\".format(analytic))\n",
    "print(\"K-L estimator: {: .5f}\".format(kozachenko))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code: https://github.com/JaeDukSeo/entropy_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
